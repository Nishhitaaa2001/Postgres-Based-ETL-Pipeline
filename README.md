 ğŸ§ Postgres-Based ETL Pipeline

A robust ETL pipeline for processing and analyzing **music streaming data** using **PostgreSQL** as the backend data warehouse.  
This project extracts, transforms, and loads song metadata and user activity logs from JSON files into a structured **star schema** for analytics and reporting.

---

## ğŸ“Œ Table of Contents
- [Project Overview](#project-overview)
- [Architecture](#architecture)
- [Data Sources](#data-sources)
- [Schema Design](#schema-design)
- [Pipeline Workflow](#pipeline-workflow)
- [How to Run](#how-to-run)
- [Project Structure](#project-structure)
- [Dependencies](#dependencies)
- [License](#license)

---

## ğŸ“– Project Overview
This project builds a scalable ETL pipeline for a sample music streaming platform (**Sparkify**).  
It processes song metadata and user activity logs stored as JSON files and loads them into PostgreSQL for faster querying, reporting, and insights into user listening behavior.

---

## ğŸ—ï¸ Architecture

| Component       | Technology |
|----------------|------------|
| Database       | PostgreSQL |
| ETL Processing | Python     |
| Data Format    | JSON       |

ETL Pipeline:
Extract â†’ Transform â†’ Load â†’ Query â†’ Analytics

yaml
Copy code

---

## ğŸ“‚ Data Sources
| Dataset | Description |
|--------|-------------|
| **Song Data** | Contains song metadata (title, artist, duration, year) |
| **User Logs** | App events recording song plays, user activity & timestamps |

---

## â­ Schema Design (Star Schema)

**Fact Table**
- `songplays` â€” records for each user song stream event

**Dimension Tables**
- `users` â€” user attributes (level, gender, name, etc.)
- `songs` â€” song details (title, duration, year)
- `artists` â€” artist metadata (name, location)
- `time` â€” timestamp breakdown (hour, day, month, year, weekday)

This structure supports **analytical queries and BI dashboards**.

---

## ğŸ”„ Pipeline Workflow
| Phase | Details |
|------|---------|
| **Extract** | Load raw JSON logs & metadata |
| **Transform** | Clean + parse timestamps + link songs & artists |
| **Load** | Insert processed data into normalized tables in PostgreSQL |

Executed using Python scripts + SQL insert queries.

---

## â–¶ï¸ How to Run

### 1ï¸âƒ£ Clone the repository
```sh
git clone <your-repo-url>
cd Postgres-Based-ETL-Pipeline
2ï¸âƒ£ Install required libraries
sh
Copy code
pip install -r requirements.txt
â€”or manually:

sh
Copy code
pip install psycopg2 pandas numpy
3ï¸âƒ£ Setup database tables
sh
Copy code
python create_tables.py
4ï¸âƒ£ Run ETL Pipeline
sh
Copy code
python etl.py
âœ… PostgreSQL database gets populated with analytics-ready tables

ğŸ“ Project Structure
graphql
Copy code
Postgres-Based-ETL-Pipeline/
â”œâ”€â”€ create_tables.py       # Creates & resets database schema
â”œâ”€â”€ etl.py                 # ETL workflow execution script
â”œâ”€â”€ sql_queries.py         # SQL for create/insert operations
â”œâ”€â”€ test.ipynb             # Notebook for validation/queries
â””â”€â”€ README.md              # Documentation
ğŸ§© Dependencies
Python 3.x

PostgreSQL

psycopg2

pandas

numpy

ğŸ“œ License
Licensed under the MIT License.
See the LICENSE file for more details.

âœ¨ Author
Nishita Rajak
Data Engineering & Analytics
ğŸ”— Feel free to â­ this repository if you find it useful!
